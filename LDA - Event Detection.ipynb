{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PreSetup\n",
    "!pip install numpy\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install seaborn\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing data\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import numpy as np\n",
    "\n",
    "# Word Tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "\n",
    "# Data Representation\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Twitter API\n",
    "from tweepy import Stream, API, OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# Utils\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to http://apps.twitter.com and create an app.\n",
    "# The consumer key and secret will be generated for you after\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "\n",
    "# After the step above, you will be redirected to your app's page.\n",
    "# Create an access token under the the \"Your access token\" section\n",
    "access_token=\"\"\n",
    "access_token_secret=\"\"\n",
    "\n",
    "# Configure the maximum number of tweets\n",
    "max_number_of_tweets = 1000\n",
    "\n",
    "# Topic Generator Settings\n",
    "topics_to_generate = 20\n",
    "words_per_topic = 7\n",
    "\n",
    "# We will use this to align the tweets\n",
    "date_format = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those are the english words that will be omitted\n",
    "to_track = [\"data\", \"artificial\",\"intelligence\", \"machile\", \"learning\", \"event\", \"detection\", \"python\"]\n",
    "default_stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"rt\", \"http\", \"wa\", \"nt\", \"re\", \"amp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NLTK Data\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryListner(StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This is a basic listener that just adds received tweets to memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, maxNumberOfTweets):\n",
    "        self.max_tweets = maxNumberOfTweets\n",
    "        self.tweet_count = 0\n",
    "        \n",
    "        self.tweets = []\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        # We want to get just a small number of tweets\n",
    "        if (self.tweet_count < self.max_tweets):\n",
    "            self.tweet_count += 1\n",
    "            \n",
    "            self.tweets.append(data)\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "    def get_tweets(self):\n",
    "        return self.tweets\n",
    "\n",
    "    def on_status(self, status):\n",
    "        print(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a listner for the Stream\n",
    "my_listner = MemoryListner(max_number_of_tweets)\n",
    "\n",
    "# Generate an authenticator\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = API(auth)\n",
    "\n",
    "# We will listen to the Stream only for english and \n",
    "# we should add some common words to look for\n",
    "my_stream = Stream(auth = api.auth, listener=my_listner)\n",
    "my_stream.filter(track=to_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use lemmatization. If you want to know more, please visit:\n",
    "# https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ED Text Cleaning\n",
    "def clean_text(text, stop_words, extra_words = []):\n",
    "        def tokenize_text(text):\n",
    "            return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "        def remove_special_characters(text):\n",
    "            tokens = tokenize_text(text)\n",
    "            return ' '.join(re.sub('[^a-z]+', '', x) for x in tokens)\n",
    "\n",
    "        def lemma_text(text, lemmatizer=wordnet_lemmatizer):\n",
    "            tokens = tokenize_text(text)\n",
    "            return ' '.join([lemmatizer.lemmatize(t) for t in tokens])\n",
    "\n",
    "        def remove_stopwords(text, stop_words= (stop_words + extra_words)):\n",
    "            tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "        text = str(text).strip(' ') # strip whitespaces\n",
    "        text = text.lower() # lowercase\n",
    "        text = remove_special_characters(text) # remove punctuation and symbols\n",
    "        text = lemma_text(text) # stemming\n",
    "        text = remove_stopwords(text) # remove stopwords\n",
    "        text = text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All tweets and prepare some list for data\n",
    "data = []\n",
    "all_tweets = my_listner.get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the tweets and take only the 2 columns of interest\n",
    "for tweet in all_tweets:\n",
    "    y = json.loads(tweet)\n",
    "    try:\n",
    "        text = clean_text(y['text'], default_stopwords)\n",
    "\n",
    "        date = parser.parse(y['created_at'])\n",
    "        datetime = date.strftime(date_format)\n",
    "\n",
    "        data.append([text, datetime])\n",
    "        \n",
    "    except Exception:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to a pandas DataFrame\n",
    "print('Number of parsed tweets = ' + str(len(data)))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text', 'date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, count_vectorizer, n_top_words):\n",
    "        words = count_vectorizer.get_feature_names()\n",
    "        data_labels = []\n",
    "\n",
    "        for _, topic in enumerate(model.components_):\n",
    "            data_labels.append(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "        topics = pd.DataFrame(data=data_labels, columns=['topic'])\n",
    "        \n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_common_10(count_data, count_vectorizer):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "        \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "        \n",
    "    figure = plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "        \n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cloud(given_text):\n",
    "    # Create a WordCloud object\n",
    "    word_cloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', width=1600, height=800)\n",
    "\n",
    "    # Generate a word cloud\n",
    "    cloud = word_cloud.generate(given_text)\n",
    "        \n",
    "    figure = plt.figure(figsize=(20,10))\n",
    "    plt.imshow(cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OLDA(topics_to_generate, words_per_topic):\n",
    "        papers = df\n",
    "\n",
    "        # Join the different processed titles together.\n",
    "        long_string = ','.join(list(str(x) for x in papers['text'].values))\n",
    "        \n",
    "        # Initialise the count vectorizer\n",
    "        count_vectorizer = CountVectorizer()\n",
    "\n",
    "        # Fit and transform the processed titles\n",
    "        count_data = count_vectorizer.fit_transform(papers['text'].values.astype('str'))\n",
    "\n",
    "        # Visualise the 10 most common words\n",
    "\n",
    "        # Create and fit the LDA model\n",
    "        lda = LDA(n_components = topics_to_generate)\n",
    "        lda.fit(count_data)\n",
    "\n",
    "        print('OLDA is done')\n",
    "        # Print the topics found by the LDA model\n",
    "        \n",
    "        topics = get_topics(lda, count_vectorizer, words_per_topic)\n",
    "        print(topics)\n",
    "        \n",
    "        print_cloud(long_string)\n",
    "        plot_most_common_10(count_data, count_vectorizer)        \n",
    "        \n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "olda_topics = train_OLDA(topics_to_generate, words_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olda_topics['set'] = olda_topics['topic'].map(lambda x: set(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def project_topics(topic_threshold):\n",
    "    magnitude = {}\n",
    "    event_start = {}\n",
    "    event_end = {}\n",
    "    fine_grained = []\n",
    "\n",
    "    # We will go trough each tweet and try to match it to a topic\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            rowSet = set(row['text'].split())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        for _, topic_row in olda_topics.iterrows():\n",
    "            topic_set = topic_row['set']\n",
    "            original_topic = topic_row['topic']\n",
    "            \n",
    "            if len(topic_set.intersection(rowSet)) > topic_threshold:\n",
    "                if original_topic not in magnitude:\n",
    "                    magnitude[original_topic] = 0\n",
    "\n",
    "                magnitude[original_topic] += 1\n",
    "                given_date = row['date']\n",
    "                fine_grained.append([original_topic, given_date, topic_threshold])\n",
    "                \n",
    "                if original_topic not in event_start:\n",
    "                    event_start[original_topic] = given_date\n",
    "                    \n",
    "                if given_date < event_start[original_topic]:\n",
    "                    event_start[original_topic] = given_date\n",
    "                    \n",
    "                if original_topic not in event_end:\n",
    "                    event_end[original_topic] = given_date\n",
    "                    \n",
    "                if given_date > event_end[original_topic]:\n",
    "                    event_end[original_topic] = given_date\n",
    "\n",
    "\n",
    "    olda_data = []\n",
    "    for _, row in olda_topics.iterrows():\n",
    "        topic = row['topic']\n",
    "        olda_data.append([topic, magnitude.get(topic, 0), event_start.get(topic, 'NULL'), event_end.get(topic, 'NULL'), topic_threshold])\n",
    "\n",
    "    return (olda_data, fine_grained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(one_word_match, fine_grained_one_word_match) = project_topics(1)\n",
    "(two_words_match, fine_grained_two_words_match) = project_topics(2)\n",
    "(three_words_match, fine_grained_three_words_match) = project_topics(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = one_word_match + two_words_match + three_words_match\n",
    "fine_grained_data = fine_grained_one_word_match + fine_grained_two_words_match + fine_grained_three_words_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "olda_df = pd.DataFrame(data, columns=['Topic', 'Magnitude', 'StartDate', 'EndDate', 'MatchSize'])\n",
    "\n",
    "print(olda_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_grained_df = pd.DataFrame(fine_grained_data, columns=['Topic', 'EventDate', 'MatchSize'])\n",
    "\n",
    "print(fine_grained_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olda_df.to_csv('output.csv', index=False)\n",
    "fine_grained_df.to_csv('output_detailed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}